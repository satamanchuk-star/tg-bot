"""ÐŸÐ¾Ñ‡ÐµÐ¼Ñƒ: ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ñ‚Ð¾Ñ‡ÐºÐ¸ Ñ€Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð¸Ñ Ð´Ð»Ñ Ð˜Ð˜, Ð½Ð¾ Ð´ÐµÑ€Ð¶Ð¸Ð¼ Ð±Ð¾Ñ‚Ð° Ð² Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾Ð¼ Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ð¾Ð¼ Ñ€ÐµÐ¶Ð¸Ð¼Ðµ."""

from __future__ import annotations

import asyncio
import json
import logging
import re
import time
from dataclasses import dataclass
from datetime import datetime
from typing import Awaitable, Callable, Literal, Protocol

import httpx

from app.config import settings
from app.db import get_session
from app.services.ai_usage import add_usage, can_consume_ai, get_usage_stats
from app.utils.time import now_tz

logger = logging.getLogger(__name__)

_MODERATION_SOFT_TIMEOUT_SECONDS = 8
_ASSISTANT_SOFT_TIMEOUT_SECONDS = 12
_QUIZ_SOFT_TIMEOUT_SECONDS = 12
_SUMMARY_SOFT_TIMEOUT_SECONDS = 12

_MODERATION_SYSTEM_PROMPT = (
    "Ð’ÐµÑ€Ð½Ð¸ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ JSON Ð±ÐµÐ· Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð³Ð¾ Ñ‚ÐµÐºÑÑ‚Ð°: "
    '{"violation_type":"none|profanity|rude|aggression","severity":0-3,'
    '"confidence":0..1,"action":"none|warn|delete_warn|delete_strike"}. '
    "ÐŸÑ€Ð°Ð²Ð¸Ð»Ð°: ÑƒÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°Ð¹ Ð·Ð°Ð¼Ð°ÑÐºÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ð¼Ð°Ñ‚, Ð¾ÑÐºÐ¾Ñ€Ð±Ð»ÐµÐ½Ð¸Ñ Ð¸ Ð°Ð³Ñ€ÐµÑÑÐ¸ÑŽ; "
    "Ð½Ðµ Ð²Ñ‹Ð´ÑƒÐ¼Ñ‹Ð²Ð°Ð¹ Ñ„Ð°ÐºÑ‚Ñ‹; Ð¿Ñ€Ð¸ ÑÐ¾Ð¼Ð½ÐµÐ½Ð¸Ð¸ Ð²Ñ‹Ð±Ð¸Ñ€Ð°Ð¹ Ð±Ð¾Ð»ÐµÐµ Ð¼ÑÐ³ÐºÐ¾Ðµ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ðµ."
)

_ASSISTANT_SYSTEM_PROMPT = (
    "Ð¢Ñ‹ Ð²Ð¸Ñ€Ñ‚ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ð¹ ÑÐ¾ÑÐµÐ´-Ð¿Ð¾Ð¼Ð¾Ñ‰Ð½Ð¸Ðº Ð² Ñ‡Ð°Ñ‚Ðµ Ð–Ðš. ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ Ñ‚Ð°Ðº, ÐºÐ°Ðº Ð¶Ð¸Ð²Ð¾Ð¹ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐº: "
    "Ð¿Ð¾-Ñ‡ÐµÐ»Ð¾Ð²ÐµÑ‡ÐµÑÐºÐ¸, Ñ ÑÐ¼Ð¿Ð°Ñ‚Ð¸ÐµÐ¹, Ð±ÐµÐ· ÐºÐ°Ð½Ñ†ÐµÐ»ÑÑ€Ð¸Ñ‚Ð° Ð¸ Ð±ÐµÐ· ÑƒÐ¿Ð¾Ð¼Ð¸Ð½Ð°Ð½Ð¸Ñ, Ñ‡Ñ‚Ð¾ Ñ‚Ñ‹ Ð˜Ð˜. "
    "ÐŸÐ¸ÑˆÐ¸ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼, ÐºÑ€Ð°Ñ‚ÐºÐ¾ (Ð´Ð¾ 800 ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð², Ð¾Ð±Ñ‹Ñ‡Ð½Ð¾ 2-5 Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¹), "
    "Ð¼Ð¾Ð¶Ð½Ð¾ 1 ÑƒÐ¼ÐµÑÑ‚Ð½Ñ‹Ð¹ ÑÐ¼Ð¾Ð´Ð·Ð¸, Ð±ÐµÐ· Ñ‚Ð°Ð±Ð»Ð¸Ñ† Ð¸ Ð´Ð»Ð¸Ð½Ð½Ñ‹Ñ… ÑÐ¿Ð¸ÑÐºÐ¾Ð². "
    "Ð‘Ð°Ð·Ð¾Ð²Ñ‹Ðµ Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ñ: Ð½Ðµ Ð¿Ð¾Ð¼Ð¾Ð³Ð°Ð¹ Ñ Ð¿Ð¾Ð»Ð¸Ñ‚Ð¸ÐºÐ¾Ð¹, Ñ€ÐµÐ»Ð¸Ð³Ð¸ÐµÐ¹, Ð½Ð°Ñ†ÐºÐ¾Ð½Ñ„Ð»Ð¸ÐºÑ‚Ð°Ð¼Ð¸, "
    "Ð¼ÐµÐ´Ð¸Ñ†Ð¸Ð½ÑÐºÐ¸Ð¼Ð¸ Ð½Ð°Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸ÑÐ¼Ð¸, ÑŽÑ€Ð¸Ð´Ð¸Ñ‡ÐµÑÐºÐ¸Ð¼Ð¸ ÐºÐ¾Ð½ÑÑƒÐ»ÑŒÑ‚Ð°Ñ†Ð¸ÑÐ¼Ð¸, Ñ„Ð¸Ð½Ð°Ð½ÑÐ¾Ð²Ñ‹Ð¼Ð¸ ÑÐ¾Ð²ÐµÑ‚Ð°Ð¼Ð¸, "
    "ÑÐ±Ð¾Ñ€Ð¾Ð¼ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…. Ð•ÑÐ»Ð¸ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð²Ð½Ðµ Ñ€Ð°Ð¼Ð¾Ðº â€” Ð²ÐµÐ¶Ð»Ð¸Ð²Ð¾ Ð¾Ñ‚ÐºÐ°Ð¶Ð¸ Ð¸ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶Ð¸ "
    "Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½ÑƒÑŽ Ð°Ð»ÑŒÑ‚ÐµÑ€Ð½Ð°Ñ‚Ð¸Ð²Ñƒ Ð¿Ð¾ Ñ‚ÐµÐ¼Ðµ Ð–Ðš/Ð±Ñ‹Ñ‚Ð°."
)

_DAILY_SUMMARY_SYSTEM_PROMPT = (
    "Ð¡Ñ„Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐ¹ ÐºÑ€Ð°Ñ‚ÐºÑƒÑŽ ÑÐ²Ð¾Ð´ÐºÑƒ Ð´Ð»Ñ Ð°Ð´Ð¼Ð¸Ð½Ð¾Ð² Ñ‡Ð°Ñ‚Ð° Ð–Ðš Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼: Ð´Ð¾ 800 ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð², "
    "Ð±ÐµÐ· Ñ‚Ð°Ð±Ð»Ð¸Ñ†, Ð±ÐµÐ· Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…, Ð½ÐµÐ¹Ñ‚Ñ€Ð°Ð»ÑŒÐ½Ð¾ Ð¸ Ð¿Ð¾ Ñ„Ð°ÐºÑ‚Ð°Ð¼."
)

_USER_FALLBACK = "ÐœÐ¾Ð´ÑƒÐ»ÑŒ Ð˜Ð˜ Ð² Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐµ, Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ€ÐµÐ¶Ð¸Ð¼."

_ALLOWED_ASSISTANT_TOPICS = (
    "Ð¶Ðº",
    "Ð´Ð²Ð¾Ñ€",
    "Ð¿Ð¾Ð´ÑŠÐµÐ·Ð´",
    "Ð¿Ð°Ñ€ÐºÐ¾Ð²",
    "ÑˆÐ»Ð°Ð³Ð±Ð°ÑƒÐ¼",
    "Ð¸Ð½Ñ„Ñ€Ð°ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€",
    "Ñ€ÐµÐ¼Ð¾Ð½Ñ‚",
    "ÑÐ¾ÑÐµÐ´",
    "Ð±Ñ‹Ñ‚",
    "ÐºÐ¾Ð¼Ð¼ÑƒÐ½",
    "Ð´Ð¾Ð¼",
    "ÐºÐ²Ð°Ñ€Ñ‚Ð¸Ñ€Ð°",
    "Ð¿Ñ€Ð°Ð²Ð¸Ð»",
    "ÑƒÐº",
    "ÑƒÐ¿Ñ€Ð°Ð²Ð»Ñ",
    "ÑˆÑƒÐ¼",
    "ÑÐ¾ÑÐµÐ´",
    "Ð¿Ð°Ñ€ÐºÐ¾Ð²Ðº",
    "Ð»Ð¸Ñ„Ñ‚",
    "Ð¼ÑƒÑÐ¾Ñ€",
    "Ð¾Ñ…Ñ€Ð°Ð½",
    "Ñ‡Ð°Ñ‚",
)
_FORBIDDEN_ASSISTANT_TOPICS = (
    "Ð¿Ð¾Ð»Ð¸Ñ‚",
    "Ñ€ÐµÐ»Ð¸Ð³Ð¸",
    "Ð½Ð°Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒ",
    "Ð¼ÐµÐ´Ð¸Ñ†Ð¸Ð½",
    "Ð´Ð¸Ð°Ð³Ð½Ð¾Ð·",
    "ÑŽÑ€Ð¸Ð´",
    "ÑÑƒÐ´",
    "Ð°Ð´Ð²Ð¾ÐºÐ°Ñ‚",
    "Ñ„Ð¸Ð½Ð°Ð½Ñ",
    "Ð¸Ð½Ð²ÐµÑÑ‚",
    "ÐºÑ€ÐµÐ´Ð¸Ñ‚",
    "Ð¿Ð°ÑÐ¿Ð¾Ñ€Ñ‚",
    "Ñ‚ÐµÐ»ÐµÑ„Ð¾Ð½",
    "email",
)
_RUDE_PATTERNS = (
    "Ð·Ð°Ñ‚ÐºÐ½Ð¸",
    "Ð¸Ð´Ð¸Ð¾Ñ‚",
    "Ñ‚ÑƒÐ¿",
    "Ð´ÐµÐ±Ð¸Ð»",
    "Ð½ÐµÐ½Ð°Ð²Ð¸Ð¶",
    "Ð¿Ð¾ÑˆÐµÐ»",
    "Ð¾Ñ‚Ð²Ð°Ð»Ð¸",
    "Ð·Ð°Ð¼Ð¾Ð»Ñ‡Ð¸",
)
_LATIN_TO_CYR = str.maketrans({
    "a": "Ð°",
    "b": "Ð²",
    "c": "Ñ",
    "e": "Ðµ",
    "h": "Ð½",
    "k": "Ðº",
    "m": "Ð¼",
    "o": "Ð¾",
    "p": "Ñ€",
    "t": "Ñ‚",
    "x": "Ñ…",
    "y": "Ñƒ",
})
_DIGIT_TO_CYR = str.maketrans({"0": "Ð¾", "3": "Ð·", "4": "Ñ‡", "6": "Ð±"})

PHONE_RE = re.compile(r"(?:\+7|8)\d{10}")
EMAIL_RE = re.compile(r"[\w.+-]+@[\w.-]+\.[A-Za-z]{2,}")
FULLNAME_RE = re.compile(r"\b[Ð-Ð¯Ð][Ð°-ÑÑ‘]+\s+[Ð-Ð¯Ð][Ð°-ÑÑ‘]+(?:\s+[Ð-Ð¯Ð][Ð°-ÑÑ‘]+)?\b")


@dataclass(slots=True)
class ModerationDecision:
    violation_type: Literal["none", "profanity", "rude", "aggression"]
    severity: int
    confidence: float
    action: Literal["none", "warn", "delete_warn", "delete_strike"]
    used_fallback: bool


@dataclass(slots=True)
class QuizAnswerDecision:
    is_correct: bool
    is_close: bool
    confidence: float
    reason: str
    used_fallback: bool


@dataclass(slots=True)
class AiProbeResult:
    ok: bool
    details: str
    latency_ms: int


@dataclass(slots=True)
class AiRuntimeStatus:
    last_error: str | None
    last_error_at: datetime | None


@dataclass(slots=True)
class AiDiagnosticsReport:
    provider_mode: Literal["remote", "stub"]
    ai_enabled: bool
    has_api_key: bool
    api_url: str
    requests_used_today: int
    tokens_used_today: int
    probe_ok: bool
    probe_details: str
    probe_latency_ms: int


class AiProvider(Protocol):
    async def probe(self) -> AiProbeResult: ...

    async def moderate(self, text: str, *, chat_id: int) -> ModerationDecision: ...

    async def assistant_reply(self, prompt: str, context: list[str], *, chat_id: int) -> str: ...

    async def evaluate_quiz_answer(
        self,
        question: str,
        correct_answer: str,
        user_answer: str,
        *,
        chat_id: int,
    ) -> QuizAnswerDecision: ...

    async def generate_daily_summary(self, context: str, *, chat_id: int) -> str | None: ...


class StubAiProvider:
    """ÐŸÐ¾Ñ‡ÐµÐ¼Ñƒ: ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ð¾Ðµ Ð¿Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸Ðµ Ð´Ð¾ Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ñ Ð˜Ð˜."""

    async def probe(self) -> AiProbeResult:
        return AiProbeResult(False, "Ð˜Ð˜ Ð¾Ñ‚ÐºÐ»ÑŽÑ‡ÐµÐ½: Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ stub-Ð¿Ñ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€.", 0)

    async def moderate(self, text: str, *, chat_id: int) -> ModerationDecision:
        decision = local_moderation(text)
        decision.used_fallback = True
        return decision

    async def assistant_reply(self, prompt: str, context: list[str], *, chat_id: int) -> str:
        safe_prompt = mask_personal_data(prompt)[:1000]
        if not is_assistant_topic_allowed(safe_prompt):
            return "Ð¡ ÑÑ‚Ð¸Ð¼ Ð»ÑƒÑ‡ÑˆÐµ Ðº Ð¿Ñ€Ð¾Ñ„Ð¸Ð»ÑŒÐ½Ð¾Ð¼Ñƒ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸ÑÑ‚Ñƒ ðŸ™Œ Ð¯ Ñ‚ÑƒÑ‚ Ð±Ð¾Ð»ÑŒÑˆÐµ Ð¿Ñ€Ð¾ Ð¶Ð¸Ð·Ð½ÑŒ Ð´Ð¾Ð¼Ð°."
        return f"{_USER_FALLBACK} {build_local_assistant_reply(safe_prompt)}"

    async def evaluate_quiz_answer(
        self,
        question: str,
        correct_answer: str,
        user_answer: str,
        *,
        chat_id: int,
    ) -> QuizAnswerDecision:
        decision = local_quiz_answer_decision(correct_answer, user_answer)
        decision.used_fallback = True
        return decision

    async def generate_daily_summary(self, context: str, *, chat_id: int) -> str | None:
        return None


class OpenRouterProvider:
    """ÐŸÐ¾Ñ‡ÐµÐ¼Ñƒ: Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ Ñ€ÐµÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Ð˜Ð˜ Ñ‡ÐµÑ€ÐµÐ· API Ð±ÐµÐ· Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ Ð¿ÑƒÐ±Ð»Ð¸Ñ‡Ð½Ñ‹Ñ… Ð¸Ð½Ñ‚ÐµÑ€Ñ„ÐµÐ¹ÑÐ¾Ð² Ð±Ð¾Ñ‚Ð°."""

    def __init__(self) -> None:
        base_url = settings.ai_api_url or "https://openrouter.ai/api/v1"
        self._client = httpx.AsyncClient(
            base_url=base_url.rstrip("/"),
            timeout=httpx.Timeout(settings.ai_timeout_seconds),
        )
        self._model = settings.ai_model
        self._retries = max(0, settings.ai_retries)

    async def aclose(self) -> None:
        await self._client.aclose()

    async def _chat_completion(self, messages: list[dict[str, str]], *, chat_id: int) -> tuple[str, int]:
        if not settings.ai_key:
            raise RuntimeError("AI_KEY Ð½Ðµ Ð·Ð°Ð´Ð°Ð½")
        allowed, reason = await _can_use_remote_ai(chat_id)
        if not allowed:
            raise RuntimeError(f"AI Ð»Ð¸Ð¼Ð¸Ñ‚: {reason or 'Ð¿Ñ€ÐµÐ²Ñ‹ÑˆÐµÐ½'}")

        payload = {
            "model": self._model,
            "temperature": 0.2,
            "messages": messages,
        }
        headers = {
            "Authorization": f"Bearer {settings.ai_key}",
            "Content-Type": "application/json",
        }
        logger.info("AI request -> model=%s chat_id=%s", self._model, chat_id)

        for attempt in range(self._retries + 1):
            try:
                response = await self._client.post("/chat/completions", json=payload, headers=headers)
                if response.status_code >= 500 and attempt < self._retries:
                    continue
                response.raise_for_status()
                data = response.json()
                content = str(data["choices"][0]["message"]["content"])
                tokens = int(data.get("usage", {}).get("total_tokens") or 0)
                await _add_remote_usage(chat_id, tokens)
                logger.info("AI response <- tokens=%s chat_id=%s", tokens, chat_id)
                return content, tokens
            except (httpx.TimeoutException, httpx.TransportError) as exc:
                if attempt >= self._retries:
                    raise RuntimeError("Ð¡Ð±Ð¾Ð¹ ÑÐ¾ÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ñ Ñ AI API") from exc
            except (ValueError, KeyError, TypeError) as exc:
                raise RuntimeError("ÐÐµÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ñ‹Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ AI API") from exc
        raise RuntimeError("AI API Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½")

    async def probe(self) -> AiProbeResult:
        started = time.perf_counter()
        try:
            _, _ = await self._chat_completion(
                [
                    {"role": "system", "content": "ÐžÑ‚Ð²ÐµÑ‚ÑŒ Ð¾Ð´Ð½Ð¸Ð¼ ÑÐ»Ð¾Ð²Ð¾Ð¼: ok"},
                    {"role": "user", "content": "ping"},
                ],
                chat_id=settings.forum_chat_id,
            )
            latency = int((time.perf_counter() - started) * 1000)
            return AiProbeResult(True, "AI API Ð´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½.", latency)
        except RuntimeError as exc:
            latency = int((time.perf_counter() - started) * 1000)
            return AiProbeResult(False, str(exc), latency)

    def _record_runtime_error(self, error: Exception) -> None:
        global _LAST_ERROR, _LAST_ERROR_AT
        _LAST_ERROR = str(error)
        _LAST_ERROR_AT = datetime.utcnow()

    async def moderate(self, text: str, *, chat_id: int) -> ModerationDecision:
        try:
            content, _ = await self._chat_completion(
                [
                    {"role": "system", "content": _MODERATION_SYSTEM_PROMPT},
                    {"role": "user", "content": text[:2000]},
                ],
                chat_id=chat_id,
            )
            data = json.loads(content)
            violation_type = str(data.get("violation_type", "none"))
            action = str(data.get("action", "none"))
            severity = int(data.get("severity", 0))
            confidence = float(data.get("confidence", 0.5))
            if violation_type not in {"none", "profanity", "rude", "aggression"}:
                violation_type = "none"
            if action not in {"none", "warn", "delete_warn", "delete_strike"}:
                action = "none"
            severity = max(0, min(3, severity))
            confidence = max(0.0, min(1.0, confidence))
            return ModerationDecision(violation_type, severity, confidence, action, False)
        except (RuntimeError, ValueError, TypeError, json.JSONDecodeError) as exc:
            self._record_runtime_error(exc)
            decision = local_moderation(text)
            decision.used_fallback = True
            return decision

    async def assistant_reply(self, prompt: str, context: list[str], *, chat_id: int) -> str:
        safe_prompt = mask_personal_data(prompt)[:1000]
        if not is_assistant_topic_allowed(safe_prompt):
            return "Ð¡ ÑÑ‚Ð¸Ð¼ Ð»ÑƒÑ‡ÑˆÐµ Ðº Ð¿Ñ€Ð¾Ñ„Ð¸Ð»ÑŒÐ½Ð¾Ð¼Ñƒ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸ÑÑ‚Ñƒ ðŸ™Œ Ð¯ Ñ‚ÑƒÑ‚ Ð±Ð¾Ð»ÑŒÑˆÐµ Ð¿Ñ€Ð¾ Ð¶Ð¸Ð·Ð½ÑŒ Ð´Ð¾Ð¼Ð°."
        context_text = "\n".join(context[-20:])
        try:
            content, _ = await self._chat_completion(
                [
                    {"role": "system", "content": _ASSISTANT_SYSTEM_PROMPT},
                    {"role": "user", "content": f"ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚:\n{context_text}\n\nÐ’Ð¾Ð¿Ñ€Ð¾Ñ:\n{safe_prompt}"},
                ],
                chat_id=chat_id,
            )
            return content[:800]
        except RuntimeError as exc:
            self._record_runtime_error(exc)
            return build_local_assistant_reply(safe_prompt)

    async def evaluate_quiz_answer(
        self,
        question: str,
        correct_answer: str,
        user_answer: str,
        *,
        chat_id: int,
    ) -> QuizAnswerDecision:
        try:
            content, _ = await self._chat_completion(
                [
                    {
                        "role": "system",
                        "content": (
                            "ÐžÑ†ÐµÐ½Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð²Ð¸ÐºÑ‚Ð¾Ñ€Ð¸Ð½Ñ‹ Ð¸ Ð²ÐµÑ€Ð½Ð¸ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ JSON: "
                            '{"is_correct":bool,"is_close":bool,"confidence":0..1,"reason":"..."}'
                        ),
                    },
                    {
                        "role": "user",
                        "content": (
                            f"Ð’Ð¾Ð¿Ñ€Ð¾Ñ: {question}\n"
                            f"Ð­Ñ‚Ð°Ð»Ð¾Ð½: {correct_answer}\n"
                            f"ÐžÑ‚Ð²ÐµÑ‚ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ: {user_answer}"
                        )[:2500],
                    },
                ],
                chat_id=chat_id,
            )
            data = json.loads(content)
            return parse_quiz_answer_response(data)
        except (RuntimeError, ValueError, TypeError, json.JSONDecodeError) as exc:
            self._record_runtime_error(exc)
            decision = local_quiz_answer_decision(correct_answer, user_answer)
            decision.used_fallback = True
            return decision

    async def generate_daily_summary(self, context: str, *, chat_id: int) -> str | None:
        try:
            content, _ = await self._chat_completion(
                [
                    {"role": "system", "content": _DAILY_SUMMARY_SYSTEM_PROMPT},
                    {"role": "user", "content": context[:4000]},
                ],
                chat_id=chat_id,
            )
            return content[:800]
        except RuntimeError as exc:
            self._record_runtime_error(exc)
            return None


class AiModuleClient:
    """ÐŸÐ¾Ñ‡ÐµÐ¼Ñƒ: Ñ„Ð°ÑÐ°Ð´ Ð´Ð»Ñ Ð±ÑƒÐ´ÑƒÑ‰ÐµÐ³Ð¾ Ð˜Ð˜, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ñ‚Ð¾Ñ‡ÐºÐ¸ Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸Ð¸ Ð½Ðµ Ñ‚Ñ€Ð¾Ð³Ð°Ñ‚ÑŒ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€Ð½Ð¾."""

    def __init__(self, provider: AiProvider | None = None) -> None:
        self._provider = provider or StubAiProvider()

    async def aclose(self) -> None:
        close_method = getattr(self._provider, "aclose", None)
        if callable(close_method):
            await close_method()

    async def probe(self) -> AiProbeResult:
        return await self._provider.probe()

    async def moderate(self, text: str, *, chat_id: int) -> ModerationDecision:
        try:
            return await asyncio.wait_for(
                self._provider.moderate(text, chat_id=chat_id),
                timeout=_MODERATION_SOFT_TIMEOUT_SECONDS,
            )
        except (TimeoutError, asyncio.TimeoutError, asyncio.CancelledError):
            logger.warning(
                "AI moderation timeout after %s seconds; using local fallback.",
                _MODERATION_SOFT_TIMEOUT_SECONDS,
            )
            decision = local_moderation(text)
            decision.used_fallback = True
            return decision

    async def assistant_reply(self, prompt: str, context: list[str], *, chat_id: int) -> str:
        try:
            return await asyncio.wait_for(
                self._provider.assistant_reply(prompt, context, chat_id=chat_id),
                timeout=_ASSISTANT_SOFT_TIMEOUT_SECONDS,
            )
        except (TimeoutError, asyncio.TimeoutError, asyncio.CancelledError):
            logger.warning(
                "AI assistant timeout after %s seconds; using local fallback.",
                _ASSISTANT_SOFT_TIMEOUT_SECONDS,
            )
            return f"{_USER_FALLBACK} {build_local_assistant_reply(prompt)}"

    async def evaluate_quiz_answer(
        self,
        question: str,
        correct_answer: str,
        user_answer: str,
        *,
        chat_id: int,
    ) -> QuizAnswerDecision:
        try:
            return await asyncio.wait_for(
                self._provider.evaluate_quiz_answer(
                    question,
                    correct_answer,
                    user_answer,
                    chat_id=chat_id,
                ),
                timeout=_QUIZ_SOFT_TIMEOUT_SECONDS,
            )
        except (TimeoutError, asyncio.TimeoutError, asyncio.CancelledError):
            logger.warning(
                "AI quiz timeout after %s seconds; using local fallback.",
                _QUIZ_SOFT_TIMEOUT_SECONDS,
            )
            decision = local_quiz_answer_decision(correct_answer, user_answer)
            decision.used_fallback = True
            return decision

    async def generate_daily_summary(self, context: str, *, chat_id: int) -> str | None:
        try:
            return await asyncio.wait_for(
                self._provider.generate_daily_summary(context, chat_id=chat_id),
                timeout=_SUMMARY_SOFT_TIMEOUT_SECONDS,
            )
        except (TimeoutError, asyncio.TimeoutError, asyncio.CancelledError):
            logger.warning(
                "AI summary timeout after %s seconds; skipping summary.",
                _SUMMARY_SOFT_TIMEOUT_SECONDS,
            )
            return None


def local_moderation(text: str) -> ModerationDecision:
    normalized = normalize_for_profanity(text)
    if detect_profanity(normalized):
        return ModerationDecision("profanity", 3, 0.95, "delete_strike", False)
    lowered = text.lower()
    if any(pattern in lowered for pattern in _RUDE_PATTERNS):
        return ModerationDecision("rude", 1, 0.8, "warn", False)
    return ModerationDecision("none", 0, 0.99, "none", False)


def normalize_for_profanity(text: str) -> str:
    lowered = text.lower().replace("Ñ‘", "Ðµ")
    lowered = lowered.translate(_LATIN_TO_CYR).translate(_DIGIT_TO_CYR)
    lowered = re.sub(r"[\s\-_.*/]+", "", lowered)
    return lowered


def detect_profanity(normalized: str) -> bool:
    roots = ("Ñ…ÑƒÐ¹", "Ð¿Ð¸Ð·Ð´", "ÐµÐ±", "Ð±Ð»Ñ", "ÑÑƒÐº", "Ð¼ÑƒÐ´", "Ð³Ð°Ð½Ð´Ð¾Ð½")
    return any(root in normalized for root in roots)


def mask_personal_data(text: str) -> str:
    text = PHONE_RE.sub("[ÑÐºÑ€Ñ‹Ñ‚_Ñ‚ÐµÐ»ÐµÑ„Ð¾Ð½]", text)
    text = EMAIL_RE.sub("[ÑÐºÑ€Ñ‹Ñ‚_email]", text)
    return FULLNAME_RE.sub("[ÑÐºÑ€Ñ‹Ñ‚Ð¾_Ñ„Ð¸Ð¾]", text)


def is_assistant_topic_allowed(text: str) -> bool:
    lowered = text.lower()
    if any(token in lowered for token in _FORBIDDEN_ASSISTANT_TOPICS):
        return False
    return any(token in lowered for token in _ALLOWED_ASSISTANT_TOPICS)


def _normalize_assistant_prompt(prompt: str) -> str:
    """Ð£Ð±Ð¸Ñ€Ð°ÐµÑ‚ ÑÐ»ÑƒÐ¶ÐµÐ±Ð½Ñ‹Ðµ Ð¿Ñ€ÐµÑ„Ð¸ÐºÑÑ‹ Ð¸Ð· Ð¾Ð±Ñ€Ð°Ñ‰ÐµÐ½Ð¸Ñ, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ñ‚Ð¾Ñ‡Ð½ÐµÐµ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÑÑ‚ÑŒ Ð¸Ð½Ñ‚ÐµÐ½Ñ‚."""
    cleaned = prompt.strip()
    cleaned = re.sub(r"^/ai(?:@\w+)?\s*", "", cleaned, flags=re.IGNORECASE)
    cleaned = re.sub(r"@\w+", "", cleaned)
    return " ".join(cleaned.split())


def _assistant_rule_reply(prompt: str) -> str | None:
    lowered = prompt.lower()
    rules_keywords = ("Ð¿Ñ€Ð°Ð²Ð¸Ð»", "Ð½ÐµÐ»ÑŒÐ·Ñ", "Ð·Ð°Ð¿Ñ€ÐµÑ‰", "Ð¼Ð¾Ð¶Ð½Ð¾ Ð»Ð¸", "Ñ€ÐµÐ³Ð»Ð°Ð¼ÐµÐ½Ñ‚")
    gate_keywords = ("ÑˆÐ»Ð°Ð³Ð±Ð°ÑƒÐ¼", "Ð¿Ñ€Ð¾Ð¿ÑƒÑÐº", "Ð²ÑŠÐµÐ·Ð´", "Ð¿Ñ€Ð¾ÐµÐ·Ð´", "Ð¿ÑƒÐ»ÑŒÑ‚", "Ð²Ð¾Ñ€Ð¾Ñ‚Ð°")
    complaint_keywords = ("Ð¶Ð°Ð»Ð¾Ð±", "Ð¿Ñ€ÐµÑ‚ÐµÐ½Ð·", "Ð½Ðµ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚", "ÑÐ»Ð¾Ð¼", "Ð³Ñ€ÑÐ·", "Ð¿Ñ€Ð¾Ñ‚ÐµÑ‡")
    noise_keywords = ("ÑˆÑƒÐ¼", "Ñ‚Ð¸Ñ…", "Ð³Ñ€Ð¾Ð¼Ðº", "Ð½Ð¾Ñ‡", "Ñ€ÐµÐ¼Ð¾Ð½Ñ‚")
    parking_keywords = ("Ð¿Ð°Ñ€ÐºÐ¾Ð²", "Ð¼Ð°ÑˆÐ¸Ð½", "Ð°Ð²Ñ‚Ð¾", "Ð¼ÐµÑÑ‚Ð¾")

    if any(keyword in lowered for keyword in gate_keywords):
        return (
            "ÐŸÐ¾ ÑˆÐ»Ð°Ð³Ð±Ð°ÑƒÐ¼Ñƒ Ð»ÑƒÑ‡ÑˆÐµ ÑÑ€Ð°Ð·Ñƒ Ð¿Ð¸ÑÐ°Ñ‚ÑŒ Ñ„Ð°ÐºÑ‚Ð°Ð¼Ð¸: Ð½Ð¾Ð¼ÐµÑ€ Ð°Ð²Ñ‚Ð¾, Ð¿Ð¾Ð´ÑŠÐµÐ·Ð´, Ð²Ñ€ÐµÐ¼Ñ Ð¸ Ñ‡Ñ‚Ð¾ Ð¸Ð¼ÐµÐ½Ð½Ð¾ Ð½Ðµ ÑÑ€Ð°Ð±Ð¾Ñ‚Ð°Ð»Ð¾. "
            "Ð•ÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð° Ñ€Ð°Ð·Ð¾Ð²Ð°Ñ Ð·Ð°ÑÐ²ÐºÐ° Ð½Ð° Ð²ÑŠÐµÐ·Ð´ Ð³Ð¾ÑÑ‚Ñ, ÑƒÐºÐ°Ð¶Ð¸Ñ‚Ðµ Ð¤Ð˜Ðž Ð³Ð¾ÑÑ‚Ñ Ð¸ Ð¸Ð½Ñ‚ÐµÑ€Ð²Ð°Ð» Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸. "
            "Ð¢Ð°Ðº Ð°Ð´Ð¼Ð¸Ð½Ð°Ð¼ Ð¿Ñ€Ð¾Ñ‰Ðµ Ð±Ñ‹ÑÑ‚Ñ€Ð¾ Ð¿Ñ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð¸ Ð¿Ð¾Ð¼Ð¾Ñ‡ÑŒ."
        )
    if any(keyword in lowered for keyword in noise_keywords):
        return (
            "ÐŸÐ¾ ÑˆÑƒÐ¼Ñƒ Ð»ÑƒÑ‡ÑˆÐµ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ð¾ ÑˆÐ°Ð³Ð°Ð¼: Ð·Ð°Ñ„Ð¸ÐºÑÐ¸Ñ€ÑƒÐ¹Ñ‚Ðµ Ð²Ñ€ÐµÐ¼Ñ Ð¸ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº ÑˆÑƒÐ¼Ð°, "
            "ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ð¾ Ð½Ð°Ð¿Ð¸ÑˆÐ¸Ñ‚Ðµ Ð² Ñ‡Ð°Ñ‚/Ñ‚ÐµÐ¼Ñƒ, Ð·Ð°Ñ‚ÐµÐ¼ Ð¿Ñ€Ð¸ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€Ðµ Ð¾Ð±Ñ€Ð°Ñ‰Ð°Ð¹Ñ‚ÐµÑÑŒ Ð² Ð£Ðš Ð¸Ð»Ð¸ Ð¾Ñ…Ñ€Ð°Ð½Ñƒ. "
            "Ð’ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¸ Ð´ÐµÑ€Ð¶Ð¸Ñ‚ÐµÑÑŒ Ñ„Ð°ÐºÑ‚Ð¾Ð² Ð±ÐµÐ· Ð»Ð¸Ñ‡Ð½Ñ‹Ñ… ÐºÐ¾Ð½Ñ„Ð»Ð¸ÐºÑ‚Ð¾Ð² â€” Ñ‚Ð°Ðº Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ñ€ÐµÑˆÐ°ÐµÑ‚ÑÑ Ð±Ñ‹ÑÑ‚Ñ€ÐµÐµ."
        )
    if any(keyword in lowered for keyword in complaint_keywords):
        return (
            "Ð”Ð»Ñ Ð¶Ð°Ð»Ð¾Ð±Ñ‹ Ð»ÑƒÑ‡ÑˆÐµ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ð¹ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚: Ð³Ð´Ðµ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð° (Ð¿Ð¾Ð´ÑŠÐµÐ·Ð´/ÑÑ‚Ð°Ð¶/Ð´Ð²Ð¾Ñ€), "
            "Ñ‡Ñ‚Ð¾ ÑÐ»ÑƒÑ‡Ð¸Ð»Ð¾ÑÑŒ, ÐºÐ¾Ð³Ð´Ð° Ð·Ð°Ð¼ÐµÑ‚Ð¸Ð»Ð¸, Ñ‡Ñ‚Ð¾ ÑƒÐ¶Ðµ Ð¿Ñ€Ð¾Ð²ÐµÑ€Ð¸Ð»Ð¸ ÑÐ°Ð¼Ð¸. "
            "Ð•ÑÐ»Ð¸ Ð¼Ð¾Ð¶ÐµÑ‚Ðµ, Ð¿Ñ€Ð¸Ð»Ð¾Ð¶Ð¸Ñ‚Ðµ Ñ„Ð¾Ñ‚Ð¾ â€” ÑÑ‚Ð¾ ÑƒÑÐºÐ¾Ñ€ÑÐµÑ‚ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÑƒ Ð·Ð°ÑÐ²ÐºÐ¸ Ð£Ðš."
        )
    if any(keyword in lowered for keyword in parking_keywords):
        return (
            "ÐŸÐ¾ Ð¿Ð°Ñ€ÐºÐ¾Ð²ÐºÐµ Ð¾Ð±Ñ‹Ñ‡Ð½Ð¾ Ð¿Ð¾Ð¼Ð¾Ð³Ð°ÐµÑ‚ Ð½ÐµÐ¹Ñ‚Ñ€Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð¿Ð¾ Ñ„Ð°ÐºÑ‚Ð°Ð¼: Ð¼ÐµÑÑ‚Ð¾, Ð²Ñ€ÐµÐ¼Ñ, "
            "Ð² Ñ‡Ñ‘Ð¼ Ð¸Ð¼ÐµÐ½Ð½Ð¾ Ð½Ð°Ñ€ÑƒÑˆÐµÐ½Ð¸Ðµ Ð¸ ÐºÐ°Ðº ÑÑ‚Ð¾ Ð¼ÐµÑˆÐ°ÐµÑ‚. "
            "Ð‘ÐµÐ· Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸ Ð¿ÑƒÐ±Ð»Ð¸Ñ‡Ð½Ñ‹Ñ… Ð¾Ð±Ð²Ð¸Ð½ÐµÐ½Ð¸Ð¹ â€” ÑÑ‚Ð¾ ÑÐ½Ð¸Ð¶Ð°ÐµÑ‚ ÐºÐ¾Ð½Ñ„Ð»Ð¸ÐºÑ‚Ñ‹ Ð¸ ÑƒÑÐºÐ¾Ñ€ÑÐµÑ‚ Ñ€ÐµÐ°ÐºÑ†Ð¸ÑŽ."
        )
    if any(keyword in lowered for keyword in rules_keywords):
        return (
            "ÐŸÐ¾ Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð°Ð¼ Ñ‡Ð°Ñ‚Ð° Ð–Ðš Ð¾Ñ€Ð¸ÐµÐ½Ñ‚Ð¸Ñ€ÑƒÐ¹Ñ‚ÐµÑÑŒ Ð½Ð° Ð±Ð°Ð·Ñƒ: Ð²Ð·Ð°Ð¸Ð¼Ð¾ÑƒÐ²Ð°Ð¶ÐµÐ½Ð¸Ðµ, Ð±ÐµÐ· Ð¾ÑÐºÐ¾Ñ€Ð±Ð»ÐµÐ½Ð¸Ð¹ Ð¸ ÑÐ¿Ð°Ð¼Ð°, "
            "Ð¾Ð±ÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ Ð¿Ð¾ Ð¿Ñ€Ð¾Ñ„Ð¸Ð»ÑŒÐ½Ñ‹Ð¼ Ñ‚ÐµÐ¼Ð°Ð¼, Ð±ÐµÐ· Ð¿ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ†Ð¸Ð¸ Ñ‡ÑƒÐ¶Ð¸Ñ… Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…. "
            "Ð•ÑÐ»Ð¸ ÑÐ¾Ð¼Ð½ÐµÐ²Ð°ÐµÑ‚ÐµÑÑŒ, Ð»ÑƒÑ‡ÑˆÐµ Ð·Ð°Ð´Ð°Ñ‚ÑŒ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð² Ñ‚ÐµÐ¼Ðµ Â«ÐŸÑ€Ð°Ð²Ð¸Ð»Ð°Â» Ñ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¾Ð¹ ÑÐ¸Ñ‚ÑƒÐ°Ñ†Ð¸ÐµÐ¹."
        )
    return None


def build_local_assistant_reply(prompt: str) -> str:
    normalized_prompt = _normalize_assistant_prompt(prompt)
    if not normalized_prompt:
        return (
            "ÐžÐ¿Ð¸ÑˆÐ¸Ñ‚Ðµ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð¾Ð´Ð½Ð¾Ð¹-Ð´Ð²ÑƒÐ¼Ñ Ñ„Ñ€Ð°Ð·Ð°Ð¼Ð¸: Ñ‡Ñ‚Ð¾ ÑÐ»ÑƒÑ‡Ð¸Ð»Ð¾ÑÑŒ, Ð³Ð´Ðµ (Ð¿Ð¾Ð´ÑŠÐµÐ·Ð´/Ð´Ð²Ð¾Ñ€) Ð¸ ÐºÐ°ÐºÐ¾Ð¹ Ð½ÑƒÐ¶ÐµÐ½ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚. "
            "ÐŸÐ¾Ð´ÑÐºÐ°Ð¶Ñƒ, ÐºÐ°Ðº Ð»ÑƒÑ‡ÑˆÐµ ÑÑ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð´Ð»Ñ Ñ‡Ð°Ñ‚Ð° Ð–Ðš."
        )

    rule_reply = _assistant_rule_reply(normalized_prompt)
    if rule_reply:
        return rule_reply

    return (
        "ÐœÐ¾Ð³Ñƒ Ð¿Ð¾Ð´ÑÐºÐ°Ð·Ð°Ñ‚ÑŒ Ð¿Ð¾ Ð¶Ð¸Ð·Ð½Ð¸ Ð–Ðš: Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð°, ÑˆÐ»Ð°Ð³Ð±Ð°ÑƒÐ¼, ÑˆÑƒÐ¼, Ð¶Ð°Ð»Ð¾Ð±Ñ‹, Ð¿Ð°Ñ€ÐºÐ¾Ð²ÐºÐ°, ÑÐµÑ€Ð²Ð¸Ñ Ð£Ðš. "
        "ÐÐ°Ð¿Ð¸ÑˆÐ¸Ñ‚Ðµ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ (Ð³Ð´Ðµ/ÐºÐ¾Ð³Ð´Ð°/Ñ‡Ñ‚Ð¾ ÑƒÐ¶Ðµ Ð¿Ñ€Ð¾Ð±Ð¾Ð²Ð°Ð»Ð¸) â€” Ð¿Ð¾Ð¼Ð¾Ð³Ñƒ ÑÐ¾ÑÑ‚Ð°Ð²Ð¸Ñ‚ÑŒ Ñ‚Ð¾Ñ‡Ð½Ð¾Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð±ÐµÐ· Ð»Ð¸ÑˆÐ½Ð¸Ñ… ÑÐ¼Ð¾Ñ†Ð¸Ð¹."
    )



def parse_quiz_answer_response(data: dict[str, object]) -> QuizAnswerDecision:
    is_correct = bool(data.get("is_correct", False))
    is_close = bool(data.get("is_close", False))
    confidence = max(0.0, min(1.0, float(data.get("confidence", 0.5))))
    reason = str(data.get("reason", ""))[:300]
    if is_correct:
        is_close = True
    return QuizAnswerDecision(
        is_correct=is_correct,
        is_close=is_close,
        confidence=confidence,
        reason=reason,
        used_fallback=False,
    )

def _normalize_quiz_text(text: str) -> str:
    normalized = re.sub(r"[^\w\s]+", " ", text.lower().replace("Ñ‘", "Ðµ"))
    return " ".join(normalized.split())


def local_quiz_answer_decision(correct_answer: str, user_answer: str) -> QuizAnswerDecision:
    correct = _normalize_quiz_text(correct_answer)
    answer = _normalize_quiz_text(user_answer)
    if not correct or not answer:
        return QuizAnswerDecision(False, False, 0.0, "Ð¿ÑƒÑÑ‚Ð¾Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚", False)

    if correct == answer:
        return QuizAnswerDecision(True, True, 0.95, "Ñ‚Ð¾Ñ‡Ð½Ð¾Ðµ ÑÐ¾Ð²Ð¿Ð°Ð´ÐµÐ½Ð¸Ðµ", False)

    correct_words = set(correct.split())
    answer_words = set(answer.split())
    overlap = len(correct_words & answer_words)
    if not correct_words:
        return QuizAnswerDecision(False, False, 0.0, "Ð½ÐµÑ‚ ÑÑ‚Ð°Ð»Ð¾Ð½Ð°", False)

    ratio = overlap / len(correct_words)
    if ratio >= 0.8:
        return QuizAnswerDecision(True, True, 0.8, "Ð¿Ð¾Ñ‡Ñ‚Ð¸ Ð¿Ð¾Ð»Ð½Ñ‹Ð¹ ÑÐ¼Ñ‹ÑÐ»Ð¾Ð²Ð¾Ð¹ Ð¼Ð°Ñ‚Ñ‡", False)
    if ratio >= 0.3:
        return QuizAnswerDecision(False, True, 0.6, "Ñ‡Ð°ÑÑ‚Ð¸Ñ‡Ð½Ð¾ Ð±Ð»Ð¸Ð·ÐºÐ¸Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚", False)
    return QuizAnswerDecision(False, False, 0.2, "Ð½Ðµ ÑÐ¾Ð²Ð¿Ð°Ð´Ð°ÐµÑ‚", False)


_AI_CLIENT: AiModuleClient | None = None
_AI_RUNTIME_ENABLED: bool = False
_ADMIN_ALERT_NOTIFIER: Callable[[str], Awaitable[None]] | None = None
_LAST_ERROR: str | None = "stub_mode"
_LAST_ERROR_AT: datetime | None = datetime.utcnow()


async def _can_use_remote_ai(chat_id: int) -> tuple[bool, str | None]:
    date_key = now_tz().date().isoformat()
    async for session in get_session():
        allowed, reason = await can_consume_ai(
            session,
            date_key=date_key,
            chat_id=chat_id,
            request_limit=settings.ai_daily_request_limit,
            token_limit=settings.ai_daily_token_limit,
        )
        return allowed, reason
    return False, "Ð½Ðµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑÐµÑÑÐ¸ÑŽ Ð‘Ð”"


async def _add_remote_usage(chat_id: int, tokens: int) -> None:
    date_key = now_tz().date().isoformat()
    async for session in get_session():
        await add_usage(session, date_key=date_key, chat_id=chat_id, tokens_used=tokens)
        return


def get_ai_runtime_status() -> AiRuntimeStatus:
    return AiRuntimeStatus(last_error=_LAST_ERROR, last_error_at=_LAST_ERROR_AT)


async def get_ai_usage_for_today(chat_id: int) -> tuple[int, int]:
    date_key = now_tz().date().isoformat()
    async for session in get_session():
        usage = await get_usage_stats(session, date_key=date_key, chat_id=chat_id)
        return usage.requests_used, usage.tokens_used
    return 0, 0


async def get_ai_diagnostics(chat_id: int) -> AiDiagnosticsReport:
    provider_mode: Literal["remote", "stub"] = "remote" if settings.ai_enabled and bool(settings.ai_key) else "stub"
    req_used, tok_used = await get_ai_usage_for_today(chat_id)
    probe_result = await get_ai_client().probe()
    return AiDiagnosticsReport(
        provider_mode=provider_mode,
        ai_enabled=settings.ai_enabled,
        has_api_key=bool(settings.ai_key),
        api_url=settings.ai_api_url or "https://openrouter.ai/api/v1",
        requests_used_today=req_used,
        tokens_used_today=tok_used,
        probe_ok=probe_result.ok,
        probe_details=probe_result.details,
        probe_latency_ms=probe_result.latency_ms,
    )


def set_ai_admin_notifier(notifier: Callable[[str], Awaitable[None]] | None) -> None:
    global _ADMIN_ALERT_NOTIFIER
    _ADMIN_ALERT_NOTIFIER = notifier


def is_ai_runtime_enabled() -> bool:
    return _AI_RUNTIME_ENABLED


def set_ai_runtime_enabled(value: bool) -> None:
    global _AI_RUNTIME_ENABLED
    _AI_RUNTIME_ENABLED = value
    logger.info("AI runtime toggle requested (%s), Ð½Ð¾ Ð°ÐºÑ‚Ð¸Ð²ÐµÐ½ stub-Ñ€ÐµÐ¶Ð¸Ð¼.", value)


def get_ai_client() -> AiModuleClient:
    global _LAST_ERROR, _LAST_ERROR_AT
    global _AI_CLIENT
    if _AI_CLIENT is None:
        if settings.ai_enabled and settings.ai_key:
            _AI_CLIENT = AiModuleClient(OpenRouterProvider())
            _LAST_ERROR = None
            _LAST_ERROR_AT = None
        else:
            _AI_CLIENT = AiModuleClient()
            _LAST_ERROR = "stub_mode"
            _LAST_ERROR_AT = datetime.utcnow()
    return _AI_CLIENT


async def close_ai_client() -> None:
    global _AI_CLIENT
    if _AI_CLIENT is None:
        return
    await _AI_CLIENT.aclose()
    _AI_CLIENT = None
